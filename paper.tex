\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{The impact of data anonymization \\ on classification models}

\author{\IEEEauthorblockN{Larissa Schaldach Cutti}
\IEEEauthorblockA{\textit{Informatics Department} \\
\textit{Federal University of Parana}\\
Curitiba, PR - Brazil \\
larissa.cutti@ufpr.br}}
\maketitle

\begin{abstract}
With the rise of data protection laws worldwide, the need to make anonymized datasets available for models has become increasingly important. This raises the hypothesis that a network might learn the characteristics of the anonymization process rather than the features of the object to be identified, potentially reducing its effectiveness in real-world situations. In this paper, the performance of three models of different depths, AlexNet, VGG, and ResNet, for the task of person classification under different anonymization techniques are analyzed. The experiments were conducted by training the networks on the same set of images of people from the Microsoft Common Objects in Context dataset, with the only difference being the type of anonymization applied, uncensored, blurring and black-box. The results were evaluated based on each model's recall on the test set composed of non-anonymized images. As a result, the networks had more difficulty classifying images containing people when trained with anonymized data. The code used in this work can be found at https://github.com/Cutti-larissa/GDPR.
\end{abstract}

\begin{IEEEkeywords}
Deep Learning, CNN, Data Privacy
\end{IEEEkeywords}

\section{Introduction}
With the evolution of technology and the widespread use of the internet, debates have emerged regarding rights and responsibilities within these environments \cite{laws_article}. In this context, data protection laws such as the GDPR - General Data Protection Regulation (European Union, 2012) \cite{GDPR}, the LGPD - General Personal Data Protection Law (Brazil, 2018)\cite{LGPD}, the APPI - Act on the Protection of Personal Information (Japan, 2020)\cite{APPI}, among others, were developed around the world. 

Although this principle is shared across most data protection laws, especially according to GDPR \cite{GDPR} all personal data belonging to an individual that is processed must comply its regulations. In other words, any data that enables the identification of a person and is subjected to operations such as collection, recording, storage, modification, use, disclosure and others, must have the individual's authorization or be anonymized so that identification from the data is no longer possible. \cite{laws_article}

For deep learning, data is essential for model training and, consequently, for proper model performance\cite{data}. In classification task using convolutional neural network, for example, the first layers learn higher-level representations that help the network distinguish between objects \cite{DL}. This makes the network highly dependent on the representativeness and quality of the training data. However, due to data protection laws, such data now requires explict authorization, complicating the data collection process, or must be anonymized, which fails to reflect real-world scenarios. %ref dos carinhas

In light of this, this work aims to investigate the impact of data anonymization on classification models, focusing on the task of person classification. Three models of varying sizes were trained in three different configurations, all starting from the same initial weights and differing only in the type of training and validation data in uncensored, censored with blurring, and censored with a black box. The findings show that training with anonymized data leads to a marked decrease in the models ability to correctly classify the objects

The remainder of this paper is organized as follows. Section II presents the related work. Section III describes the experimental protocol, introducing the dataset, models, and evaluation metrics. The dataset preparation is presented in section IV. Section V discusses the experiments and results. Finally, conclusion and directions for future work are in section VI.

\section{Related Works}
The topic of data anonymization in machine learning and deep learning has gained prominence with the advent of data protection laws worldwide.

In \cite{Federated} the authors explore the advantages of using deep federated learning (DFL) for the Internet of Things (IoT), analyzing different types of DFL based on learning objectives, security and privacy aspects, data heterogeneity and learning strategy. They also discuss real-world applications, security issues and current research challenges. Most importantly, they present a general architecture designed to be compliant with the GDPR.

In \cite{Microsoft}, the authors propose a framework that injects spurious correlations into natural language datasets, preventing models from learning the original patterns without altering the semantic context. This approach is intended to protect individuals personal data, since large language models developed in recent years have been trained on publicly available content from forums and articles without the consent of the individuals who produced it, thereby violating global data protection regulations.

In \cite{dataLeak} the authors analyze different scenarios to assess whether data anonymization is sufficient in the context of machine learning and data privacy. They argue that the data should be anonymized before being published for use in machine learning applications.

When combining the context of data protection with the task of classifying people, it becomes necessary to anonymize the characteristics that enable personal identification. As a result, certain features, such as the individual's face, must be removed, which may compromise the model since it will have fewer features to learn from. This issue is the focus of the analysis presented in this work. 

\section{Experiment Protocol}

\subsection{Dataset}
The Microsoft Common Objects in Context (MS Coco) dataset \cite{coco} contains images of complex everyday scenes featuring common objects in their natural context. The images were collected from Flickr, which hosts photos uploaded by amateur photographers along with searchable metadata and keywords. To obtain more non-iconic images, meaning images that do not focus on a single class, the dataset authors searched for pairwise categories to ensure greater scene diversity. 

MS Coco is a large dataset widely used for training and evaluating deep learning models \cite{c1} \cite{c2} \cite{c3}. Although it is most commonly applied to tasks such as object detection \cite{cd1} \cite{cd2}, instance segmentation \cite{ci1} \cite{ci2} and keypoints detection \cite{ck1} \cite{ck2}, it was chosen for this work due its size, wide variety of classes (80 object categories), the diversity of images context, and its keypoints annotations, which were essential for the anonymization step. 

This paper uses the 2017 version of the dataset, which includes approximately 118k training images, 5k validation images, and 20k test images.

\subsection{Models}
To conduct the experiments, three networks of different depths were selected to analyze how the impact of anonymization scales with model complexity.

AlexNet \cite{alex} was chosen to represent small-scale architectures due to its shallow structure. For intermediate-scale networks, the VGG16 \cite{vgg} model was selected, providing greater depth, but maintaining a structure similar to the small one. ResNet18 \cite{resnet} was chosen as the representative of large-scale architectures because its residual connections should enable deeper feature extraction. Additional information about the models is provided in Table \ref{tab1}.

\begin{table}[htbp]
\centering
\caption{Models informations}\label{tab1}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} 
& \textbf{\shortstack{Convolutional \\ Layers}} 
& \textbf{\shortstack{Classification \\ Layers}} 
& \textbf{Parameters} \\
\hline
AlexNet   & 5  & 3 & 57,012,034  \\
\hline
VGG16     & 13 & 3 & 134,268,738 \\
\hline
ResNet18  & 20 & 1 & 11,172738  \\
\hline
\end{tabular}
\end{table}

\subsection{Evaluation Metrics} 
This work aims to evaluate whether the model learned the characteristics of anonymization rather than the target task itself. To this end, the recall of the model is examined. The recall metric \eqref{recall} measures the proportion of actual positive instances that are correctly identified, where 'TP' denotes 'true positives', in the context of this paper, instances that belong to the person class and were correctly classified, and 'FN' denotes 'false negatives', instances that belong to the person class but were classified as not belonging to it. Using this technique, it is possible to observe how many instances of people the model was able to identify and thus verify if the model learned the characteristics of the expected object.

\begin{equation}
Recall = \frac{TP}{TP + FN}
\label{recall} 
\end{equation}

\section{Dataset preparation}
To represent the person class who had identifiable features, such as a visible face, all images with keypoint annotations (e.g., nose, eyes, and ears) were selected. This resulted in 51,586 training images and 2,132 validation images.

After this initial filtering, the downloaded images were cropped by instance and by size, with only those larger than 1500px remaining, and any image in which the individual's face was not visible was removed. This process resulted in 42,382 training images and 2,101 validation images. 

Subsequently, the original images were filtered to retain only those that contained at least one instance remaining in the cropped-image set, resulting in 28,764 training images and 1,363 validation images.

To assess whether the model could distinguish between non-person elements, the class 'n\_person' was created. Images from MS Coco were filtered from 67 different classes\footnote{This new class was a combination of all the remaining classes except 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'kite' and 'clock'} that did not include annotations for the 'person' class. This resulted in 27,211 training images and 1,159 validation images.

Since the original test images lacked annotations and the platform provided for model validation did not support classification, the test set was removed. Therefore, the training images were redistributed, with 24,000 used for training and 4,764 for testing the 'person' class, and 24,000 for training and 3,211 for testing the 'n\_person' class, enabling the evaluation of the experiments. Overall, the datasets were divided into approximately 80\% training, 15\% testing, and 5\% validation.

For the anonymization process, two copies of each image from the training and validation set of the 'person' class were created, one censored using gaussian blur filter with a kernel size of 51, and one using a black box. Both anonymization methods were applied to the face region based on the keypoint annotations.

\section{Experiment and Results}
For the experiments conducted in this work, each configuration was repeated three times\footnote{Up to the date of submission of this work, it was not possible to perform the experiment three times on the VGG16 network.}. Each model was first trained on uncensored data, and then, using the same weight initialization, it was trained with censored versions of the data, one with blurred and another with black box. The purpose of this procedure was to approximate the scenario of the data protection regulations, in which only the data which enables the identification of the individual is anonymized.
The models were trained for 100 epochs with an early-stopping patience of 5 epochs, using a batch size of 32.

After completing the experiments, the results indicated that the networks showed greater difficulty in classifying images belonging to the 'person' class when trained with anonymized data, regardless of model size. This can be observed in Table \ref{tab2}, which reports the average recall obtained in each experiment.

\begin{table}[htbp]
\caption{Average recall per experiment}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Modelo} & \textbf{Uncensored} & \textbf{Blur} & \textbf{Black-Box} \\
\hline
AlexNet & 0,8825 & 0,1021 & 0,0042 \\
\hline
VGG & 0,9311 & 0.0300 & 0.0046 \\
\hline
ResNet18 & 0,8795 & 0,0163 & 0,0056 \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}

\section{Conclusion and Future Works}
In this work, the impact of different anonymization methods on classification models of varying depths was experimentally evaluated. From the results obtained, it is observed that all networks showed a worse performance when anonymized data was used for training. This findings suggests that additional strategies are needed to improve learning so it does not become biased toward the characteristics introduced by the anonymization process.

For future work, this analysis can be extended to detection networks, as well as complemented with further experiments, such as applying anonymization to the test data of the 'n\_person' class to observe whether the models would incorrectly classify these instances as 'person'. 

Another important direction is to investigate how to mitigate this undesired learning behavior, especially considering that data protection laws are essential for safeguarding citizens rights and will remain a significant and lasting requirement.

\begin{thebibliography}{00} %preciso arrumar as refs
\bibitem{laws_article}
L. N. Lorenzon, "Análise comparada ente regulamentações de dados pessoais no Brasil e na União Europeia (LGPD e GDPR) e seus respectivos instrumentos de enforcement" \textit{Revista de Direito Público da Economia}, May 2021. [Online] Available: https://periodicos.fgv.br/rpdue/article/view/83423

\bibitem{GDPR}
European Union, \textit{General Data Protection Regulation”}, Apr 2016. 

\bibitem{LGPD}
Brazil, \textit{General Personal Data Protection Law}, Ago 2018. 

\bibitem{APPI}
Japan, \textit{Amended Act on the Protection of Personal Information},  March 2020. 

\bibitem{data}
A. Munappy, J. Bosch, H. H. Olsson, A. Arpteg and B. Brinne, "Data Management Challenges for Deep Learning" \textit{45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, Kallithea, Greece, 2019, pp. 140-147, doi: 10.1109/SEAA.2019.00030.

\bibitem{DL}
LeCun Y, Bengio Y, Hinton G. Deep learning. \textit{Nature}, May 2015. doi: 10.1038/nature14539.

\bibitem{Federated}
Z. Abbas, S. F. Ahmad, M. H. Syed, A. Anjum and S. Rehman, "Exploring Deep Federated Learning for the Internet of Things: A GDPR-Compliant Architecture" \textit{IEEE Access}, vol. 12, pp. 10548-10574, 2024, doi: 10.1109/ACCESS.2023.3344029

\bibitem{Microsoft}
A. Java, S. Shahid, and C. Agarwal, "Towards Operationalizing Right to Data Protection" \textit{Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, Albuquerque, NM, USA, 2025,  pp. 8191–8205.

\bibitem{dataLeak}
N. Senavirathne and V. Torra, "On the Role of Data Anonymization in Machine Learning Privacy" \textit{2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)}, Guangzhou, China, 2020, pp. 664-675, doi: 10.1109/TrustCom50675.2020.00093.

\bibitem{airbone}
I. Kurmi, D. C. Schedl, and O. Bimber, "Combined person classification with airborne optical sectioning" \textit{ Scientific Reports}, vol. 12, no. 3804, 2022. [Online]. Available: https://doi.org/10.1038/s41598-022-07733-z

\bibitem{rescue}
S. Sambolek and M. Ivašić-Kos, "Automatic Person Detection in Search and Rescue Operations Using Deep CNN Detectors" \textit{IEEE Access}, vol. 9, pp. 37905-37922, 2021, doi: 10.1109/ACCESS.2021.3063681.

\bibitem{car}
J. Ni, K. Shen, Y. Chen, W. Cao and S. X. Yang, "An Improved Deep Network-Based Scene Classification Method for Self-Driving Cars" \textit{IEEE Transactions on Instrumentation and Measurement}, vol. 71, pp. 1-14, 2022, Art no. 5001614, doi: 10.1109/TIM.2022.3146923.

\bibitem{coco}
T.-Y. Lin et al. "Microsoft COCO: Common Objects in Context", \textit{Computer Vision – ECCV 2014}, D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds. Cham: Springer, 2014, pp. 740-755, doi: 10.1007/978-3-319-10602-1\_48 4.

\bibitem{c1}
A. Wang, H. Chen, L. Liu. et al. "YOLOv10: Real-Time End-to-End Object Detection", \textit{Advances in Neural Information Processing Systems 37 (NeurIPS 2024), Main Conference Track}, Vancouver, Canada, Dec. 2024, doi: 10.52202/079017-3429

\bibitem{c2}
L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, "Depth Anything: Unleashing the power of large-scale unlabeled data" \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 10371-10381 

\bibitem{c3}
X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen,  "MMMU: A massive multi-discipline multimodal understading and reasoning benchmark for expert AGI", \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024, pp. 9556-9567

\bibitem{cd1}
M. Kisantal, Z. Wojna, J. Murawski, J. Naruniec, and K. Cho, "Augmentation for small object detection" Feb 2019. [Online] Available: https://arxiv.org/abs/1902.07296

\bibitem{cd2}
K. Sohn et al. "A Simple Semi-Supervised Learning Framework for Object Detection" Dec 2020. Available: https://arxiv.org/abs/2005.04757

\bibitem{ci1}
D. Huynh, J. Kuen, Z. Lin, J. Gu, E. Elhamifar, "Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling" \textit{ Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 7020-7031 

\bibitem{ci2}
J. Cao, Y. Pang, R. M. Anwer, H. Cholakkal, F. S. Khan and L. Shao, "SipMaskv2: Enhanced Fast Image and Video Instance Segmentation" \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 45, no. 3, pp. 3798-3812, 1 March 2023, doi: 10.1109/TPAMI.2022.3180564.

\bibitem{ck1}
J. Docekal, J. Rozlivek, J. Matas and M. Hoffmann, "Human Keypoint Detection for Close Proximity Human-Robot Interaction" \textit{IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids)}, Ginowan, Japan, 2022, pp. 450-457, doi: 10.1109/Humanoids53995.2022.10000133.

\bibitem{ck2}
D. Maji, S. Nagori, M. Mathew, and D. Poddar, "YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss" \textit{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, 2022, pp. 2637-2646 

\bibitem{alex}
A. Krizhevsky Google, “One weird trick for parallelizing convolutional neural networks”, 2014. [Online] Available: https://arxiv.org/abs/1404.5997

\bibitem{vgg}
K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for large-scale image recognition”, Apr 2015. [Online] Available: https://arxiv.org/abs/1409.1556

\bibitem{resnet}
K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” Dec 2015. [Online] Available: https://arxiv.org/abs/1512.03385
‌
\end{thebibliography}
\end{document}
